---
title: "clusterExplainR-example"
output: rmarkdown::html_vignette
description: >
  Provides a brief summary of the explainer's utility and functionality. Utilizes a small dataset included in the library, which features manually assigned clusters exclusively for demonstration and illustrative purposes.
vignette: >
  %\VignetteIndexEntry{explanaier-example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# The clusterExplainR package

The clusterExplainR package is a valuable tool for explaining data clustering, designed as a post-hoc method to enhance exploratory data analysis during clustering tasks. It has the unique capability to elucidate clustering in mixed data scenarios, making it suitable for discrete, continuous, and mixed data clustering. The core concept is feature importance, computed by contrasting the entropy similarities within a cluster with those across all clusters. This explainer offers two primary insights: first, it provides a global perspective on feature importance by calculating and summarizing the most crucial features for the entire clustering based on mean, max, and min importance scores. Second, it generates local explanations for individual clusters, shedding light on what makes each cluster unique compared to the overall population. These local explanations include visually and verbally describing features, and they also incorporate rules aimed at encompassing as many instances as possible while maintaining precision.

```{r setup, warning = FALSE}
library(clusterExplainR)
```
## Loading a dataset with clustering

The clusterExplainR package contains a very small dataset for demonstrative purpose with manually assigned clusters. These clusters are assigned in such a way that the clusterExplainR explanations can be demonstrated. The clustering itself is not a "good" clustering!
The data consists of one categorical and one numerical feature/column:

1. Gender: categorical feature either Male or Female
2. Weight: numerical feature

```{r}
data <- clusterExplainR::example_mocked_clustering
data_without_clusters <- data[c(1, 2)]
clustering <- data[[3]]
head(data, 2)
```

## Global Explanation

To begin with, the ClusterExplainR package generates a global explanation. To accomplish this, it requires access to the original dataset on which the clustering was performed, as well as the clustering results. Additionally, it needs information about which columns in the dataset are either numerical or categorical. It's worth emphasizing that for optimal human comprehension, the package operates on the provided feature spaces. If the clustering was carried out on a preprocessed version of the data, using the processed feature space could make the explanations less intuitive for outsiders, including the user. Therefore, utilizing the raw data is recommended to ensure clear and understandable explanations.

```{r}
global_exp <- explain_global_clustering(
  data = data_without_clusters, 
  clusterings = clustering,
  numerical_columns = c(2), # the 2nd column is numerical 'Weight'
  categorical_columns = c(1) # the 1st column is categorical 'Gender'
)
```

### Global Feature Importance

The global explanation generated by ClusterExplainR offers a comprehensive view of feature importance scores across all clusters. By examining the mean, minimum, and maximum importance scores, users can gain insight into which features are crucial for the clustering process. These importance scores fall within the range of 0 to 1, where a score of 1 signifies that the feature is highly important (as it has only one distinct value), while a score of 0 suggests that the feature is unimportant, as its entropy does not deviate significantly from that of all the features in the dataset. This scale provides a clear indication of the relative significance of each feature in the context of the clustering analysis.

```{r}
global_exp$feature_importance
global_exp$fis_plot
```

### Global Feature Visualisation

Furthermore, the global explanation offers a function that allows users to visualize the distributions of the most important features, providing insights into how these critical features behave within the clustering. The "type" parameter can be configured with values such as 'mean,' 'min,' or 'max,' corresponding to the importance score aggregation methods as displayed in the feature importance table. This visualization function helps users gain a deeper understanding of the behavior and characteristics of the most influential features, aiding in the interpretation of clustering results.

```{r}
global_exp$top_n_features(2, type = 'mean')
```

In summary, the global explanation provided by clusterExplainR highlights the significance of the "Gender" feature in describing the clusters. However, it's important to note that the minimum importance score of 0 suggests that there is at least one cluster where this feature does not contribute to cluster description at all. Upon further examination through visualization, it becomes evident that Cluster 3 exclusively comprises male individuals, making the "Gender" feature highly relevant for this cluster. Cluster 1 is also influenced by this feature, as it consists of 1 male and 7 females. In contrast, for Cluster 2, the "Gender" feature is not relevant, as this cluster comprises an equal number of females and males. This nuanced understanding of feature importance within individual clusters enhances the interpretability of the clustering results.

## Local Cluster Explanations

Local explanations in clusterExplainR are designed to provide deeper insights into specific clusters. They serve multiple purposes:

- Identify Key Features: Local explanations aim to reveal the most important features within a given cluster. This helps users understand which aspects of the data are particularly relevant for characterizing that specific cluster.

- Inspect Feature Behavior: Users can delve into individual features to explore their behavior within the cluster. This allows for a more granular understanding of how each feature contributes to the cluster's profile.

- Instance Relevance: Local explanations provide an indication of how well each instance fits into the cluster based on the importance of features. This helps users assess the suitability of individual data points within the context of the cluster. The measure is called the EntityClusterMatchingScore (ECMS).

- Cluster Representatives: The explanations can create a set of cluster representatives, which are chosen based on the highest ECMS and cluster membership. These representatives offer a short overview of entities within the cluster.

- Descriptive Rules: Local explanations also provide descriptive rules for the cluster. These rules are generated with the aim of striking a balance between accuracy and coverage. In other words, they aim to describe the cluster as accurately as possible while encompassing as many instances as feasible, recognizing the trade-off between precision and inclusivity.

```{r}
explanation_cl_3 <- explain_cluster(
  data = data_without_clusters,
  clusterings= clustering,
  numerical_columns = c(2), # the 2nd column is numerical 'Weight'
  categorical_columns = c(1), # the 1st column is categorical 'Gender'
  cluster = 1, # we want to generate rules for a specific cluster
  generate_rules = TRUE # indicate whether or not to create descriptive rules
)
```

### Local Feature Importance

The local feature importance scores provide insights into the significance of individual features within the clustering process. As an example, for Cluster 3, we observe that "Gender" is the most critical feature, with an importance score of 1, indicating its paramount importance in defining this cluster. Following closely is the "weight" feature, with an importance score of approximately 0.6, signifying its significant but slightly lesser contribution to the cluster's characterization.

```{r}
explanation_cl_3$feature_importance
```

### Visualisations of Features

Cluster explanations visualizations comprise two distinct functions.

The first function, "Inspect Feature" takes a feature or column name as input and generates a plot illustrating how the values within that feature are distributed compared to the overall population. Additionally, it provides a verbal description that articulates the nature of these feature distributions, aiding in the interpretation of their significance.

The second function is designed to showcase the "n" most important features within the cluster. This function helps users quickly identify and focus on the key features that play a significant role in defining the cluster's characteristics.

```{r}
gender_exploration <- explanation_cl_3$inspect_feature('Gender')
gender_exploration$plot
cat(gender_exploration$verbalisation)
```

```{r}
explanation_cl_3$top_n_features(2)
```

### Entity Cluster Matching Score

Cluster Membership relevance is quantified by the ECMS (Exemplar Cluster Membership Score), which indicates how well an individual instance aligns with a specific cluster. The ECMS values are computed for all instances within the cluster, resulting in a vector that reflects the degree of fit for each instance.

```{r}
explanation_cl_3$ECMS
```

Additionally, the package includes a "cluster_representative" function, which serves to identify the "n" entries within the cluster that exhibit the highest ECMS scores among all instances assigned to that cluster. These representatives are particularly informative as they encapsulate the most typical or exemplary cases within the cluster, helping users gain a clearer understanding of its characteristics.

```{r}
explanation_cl_3$cluster_representative(1)
```

### Descriptive Rules

Finally, the package offers descriptive rules that are generated through an optimization process based on the F1 score. This optimization strategy results in rules that strike a balance between accuracy and coverage, effectively navigating the trade-off between providing precise descriptions of the cluster while ensuring inclusivity by covering as many instances as possible. These rules offer a concise yet informative way to characterize and understand the cluster's key attributes and properties.

```{r}
rule_cl_3 <- explanation_cl_3$rule
cat(rule_cl_3$verbalization)
```

